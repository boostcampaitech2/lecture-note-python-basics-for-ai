{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor ve Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import Tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLiner(nn.Module):\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        \n",
    "        self.weights = nn.Parameter(\n",
    "                torch.randn(in_features, out_features))\n",
    "        \n",
    "        self.bias = nn.Parameter(torch.randn(out_features))\n",
    "\n",
    "    def forward(self, x : Tensor):\n",
    "        return x @ self.weights + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(5, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = MyLiner(7, 12)\n",
    "layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for value in layer.parameters():\n",
    "    print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLiner(nn.Module):\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super().__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        \n",
    "        self.weights = Tensor(\n",
    "                torch.randn(in_features, out_features))\n",
    "        \n",
    "        self.bias = Tensor(torch.randn(out_features))\n",
    "\n",
    "    def forward(self, x : Tensor):\n",
    "        return x @ self.weights + self.bias\n",
    "\n",
    "\n",
    "layer = MyLiner(7, 12)\n",
    "layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for value in layer.parameters():\n",
    "    print(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoGrad for Linear Regression\n",
    "https://towardsdatascience.com/linear-regression-with-pytorch-eb6dedead817"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ y=2x+1 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# create dummy data for training\n",
    "x_values = [i for i in range(11)]\n",
    "x_train = np.array(x_values, dtype=np.float32)\n",
    "x_train = x_train.reshape(-1, 1)\n",
    "\n",
    "y_values = [2*i + 1 for i in x_values]\n",
    "y_train = np.array(y_values, dtype=np.float32)\n",
    "y_train = y_train.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "class LinearRegression(torch.nn.Module):\n",
    "    def __init__(self, inputSize, outputSize):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        self.linear = torch.nn.Linear(inputSize, outputSize)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputDim = 1        # takes variable 'x' \n",
    "outputDim = 1       # takes variable 'y'\n",
    "learningRate = 0.01 \n",
    "epochs = 100\n",
    "\n",
    "model = LinearRegression(inputDim, outputDim)\n",
    "##### For GPU #######\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.MSELoss() \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learningRate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    # Converting inputs and labels to Variable\n",
    "    if torch.cuda.is_available():\n",
    "        inputs = Variable(torch.from_numpy(x_train).cuda())\n",
    "        labels = Variable(torch.from_numpy(y_train).cuda())\n",
    "    else:\n",
    "        inputs = Variable(torch.from_numpy(x_train))\n",
    "        labels = Variable(torch.from_numpy(y_train))\n",
    "\n",
    "    # Clear gradient buffers because we don't want any gradient from previous epoch to carry forward, dont want to cummulate gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # get output from the model, given the inputs\n",
    "    outputs = model(inputs)\n",
    "\n",
    "    # get loss for the predicted output\n",
    "    loss = criterion(outputs, labels)\n",
    "    print(loss)\n",
    "    # get gradients w.r.t to parameters\n",
    "    loss.backward()\n",
    "\n",
    "    # update parameters\n",
    "    optimizer.step()\n",
    "\n",
    "    print('epoch {}, loss {}'.format(epoch, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad(): # we don't need gradients in the testing phase\n",
    "    if torch.cuda.is_available():\n",
    "        predicted = model(Variable(torch.from_numpy(x_train).cuda())).cpu().data.numpy()\n",
    "    else:\n",
    "        predicted = model(Variable(torch.from_numpy(x_train))).data.numpy()\n",
    "    print(predicted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in model.parameters():\n",
    "    if p.requires_grad:\n",
    "         print(p.name, p.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, models, transforms\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "## print out the pytorch version used (1.31 at the time of this tutorial)\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## configuration to detect cuda or cpu\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print (device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"https://download.pytorch.org/tutorial/hymenoptera_data.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import os\n",
    "import shutil\n",
    "from zipfile import ZipFile\n",
    "\n",
    "\n",
    "urllib.request .urlretrieve(DATA_PATH, \"hymenoptera_data.zip\")\n",
    "\n",
    "\n",
    "\n",
    "with ZipFile(\"hymenoptera_data.zip\", 'r') as zipObj:\n",
    "   # Extract all the contents of zip file in current directory\n",
    "   zipObj.extractall()\n",
    "\n",
    "os.rename(\"hymenoptera_data\", \"data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## configure root folder on your gdrive\n",
    "data_dir = \"./data\"\n",
    "\n",
    "## custom transformer to flatten the image tensors\n",
    "class ReshapeTransform:\n",
    "    def __init__(self, new_size):\n",
    "        self.new_size = new_size\n",
    "\n",
    "    def __call__(self, img):\n",
    "        result = torch.reshape(img, self.new_size)\n",
    "        return result\n",
    "\n",
    "## transformations used to standardize and normalize the datasets\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize(224),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        ReshapeTransform((-1,)) # flattens the data\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize(224),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        ReshapeTransform((-1,)) # flattens the data\n",
    "    ]),\n",
    "}\n",
    "\n",
    "## load the correspoding folders\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                          data_transforms[x])\n",
    "                  for x in ['train', 'val']}\n",
    "\n",
    "## load the entire dataset; we are not using minibatches here\n",
    "train_dataset = torch.utils.data.DataLoader(image_datasets['train'],\n",
    "                                            batch_size=len(image_datasets['train']),\n",
    "                                            shuffle=True)\n",
    "test_dataset = torch.utils.data.DataLoader(image_datasets['val'],\n",
    "                                           batch_size=len(image_datasets['val']),\n",
    "                                           shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load the entire dataset\n",
    "x, y = next(iter(train_dataset))\n",
    "\n",
    "## print one example\n",
    "dim = x.shape[1]\n",
    "print(\"Dimension of image:\", x.shape, \"\\n\", \n",
    "      \"Dimension of labels\", y.shape)\n",
    "\n",
    "plt.imshow(x[160].reshape(1, 3, 224, 224).squeeze().T.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LR(nn.Module):\n",
    "    def __init__(self, dim, lr=torch.scalar_tensor(0.01)):\n",
    "        super(LR, self).__init__()\n",
    "        # intialize parameters\n",
    "        self.w = torch.zeros(dim, 1, dtype=torch.float).to(device)\n",
    "        self.b = torch.scalar_tensor(0).to(device)\n",
    "        self.grads = {\"dw\": torch.zeros(dim, 1, dtype=torch.float).to(device),\n",
    "                      \"db\": torch.scalar_tensor(0).to(device)}\n",
    "        self.lr = lr.to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        ## compute forward\n",
    "        z = torch.mm(self.w.T, x) + self.b\n",
    "        a = self.sigmoid(z)\n",
    "        return a\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        return 1/(1 + torch.exp(-z))\n",
    "\n",
    "    def backward(self, x, yhat, y):\n",
    "        ## compute backward\n",
    "        self.grads[\"dw\"] = (1/x.shape[1]) * torch.mm(x, (yhat - y).T)\n",
    "        self.grads[\"db\"] = (1/x.shape[1]) * torch.sum(yhat - y)\n",
    "    \n",
    "    def optimize(self):\n",
    "        ## optimization step\n",
    "        self.w = self.w - self.lr * self.grads[\"dw\"]\n",
    "        self.b = self.b - self.lr * self.grads[\"db\"]\n",
    "\n",
    "## utility functions\n",
    "def loss(yhat, y):\n",
    "    m = y.size()[1]\n",
    "    return -(1/m)* torch.sum(y*torch.log(yhat) + (1 - y)* torch.log(1-yhat))\n",
    "\n",
    "def predict(yhat, y):\n",
    "    y_prediction = torch.zeros(1, y.size()[1])\n",
    "    for i in range(yhat.size()[1]):\n",
    "        if yhat[0, i] <= 0.5:\n",
    "            y_prediction[0, i] = 0\n",
    "        else:\n",
    "            y_prediction[0, i] = 1\n",
    "    return 100 - torch.mean(torch.abs(y_prediction - y)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## model pretesting\n",
    "x, y = next(iter(train_dataset))\n",
    "\n",
    "## flatten/transform the data\n",
    "x_flatten = x.T\n",
    "y = y.unsqueeze(0) \n",
    "\n",
    "## num_px is the dimension of the images\n",
    "dim = x_flatten.shape[0]\n",
    "\n",
    "## model instance\n",
    "model = LR(dim)\n",
    "model.to(device)\n",
    "yhat = model.forward(x_flatten.to(device))\n",
    "yhat = yhat.data.cpu()\n",
    "\n",
    "## calculate loss\n",
    "cost = loss(yhat, y)\n",
    "prediction = predict(yhat, y)\n",
    "print(\"Cost: \", cost)\n",
    "print(\"Accuracy: \", prediction)\n",
    "\n",
    "## backpropagate\n",
    "model.backward(x_flatten.to(device), yhat.to(device), y.to(device))\n",
    "model.optimize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## hyperparams\n",
    "costs = []\n",
    "dim = x_flatten.shape[0]\n",
    "learning_rate = torch.scalar_tensor(0.0001).to(device)\n",
    "num_iterations = 100\n",
    "lrmodel = LR(dim, learning_rate)\n",
    "lrmodel.to(device)\n",
    "\n",
    "## transform the data\n",
    "def transform_data(x, y):\n",
    "    x_flatten = x.T\n",
    "    y = y.unsqueeze(0) \n",
    "    return x_flatten, y \n",
    "\n",
    "## training the model\n",
    "for i in range(num_iterations):\n",
    "    x, y = next(iter(train_dataset))\n",
    "    test_x, test_y = next(iter(test_dataset))\n",
    "    x, y = transform_data(x, y)\n",
    "    test_x, test_y = transform_data(test_x, test_y)\n",
    "\n",
    "    # forward\n",
    "    yhat = lrmodel.forward(x.to(device))\n",
    "    cost = loss(yhat.data.cpu(), y)\n",
    "    train_pred = predict(yhat, y)\n",
    "        \n",
    "    # backward\n",
    "    lrmodel.backward(x.to(device), \n",
    "                    yhat.to(device), \n",
    "                    y.to(device))\n",
    "    lrmodel.optimize()\n",
    "    ## test\n",
    "    yhat_test = lrmodel.forward(test_x.to(device))\n",
    "    test_pred = predict(yhat_test, test_y)\n",
    "\n",
    "    if i % 10 == 0:\n",
    "        costs.append(cost)\n",
    "\n",
    "    if i % 10 == 0:\n",
    "        print(\"Cost after iteration {}: {} | Train Acc: {} | Test Acc: {}\".format(i, \n",
    "                                                                                    cost, \n",
    "                                                                                    train_pred,\n",
    "                                                                                    test_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## the trend in the context of loss\n",
    "plt.plot(costs)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
